{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "# Set your working directory\n",
    "WORKING_DIR = 'C:/Users/HuyenNguyen/Dropbox (Erasmus Universiteit Rotterdam)/Hamburg/TEACHING_UHH/WiSo21-22/Text Analysis for Social Sciences in Python/Exercises/W9'\n",
    "import os\n",
    "os.chdir(WORKING_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import relevant packages\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "import re\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pydot'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-67f4c497571a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpydot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpyd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSVG\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvis_utils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodel_to_dot\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pydot'"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import pydot as pyd\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the csv file\n",
    "df1 = pd.read_csv('death-penalty-cases.csv')\n",
    "df1 = df1[pd.notnull(df1['author_id'])] # drop cases without an author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>court_id</th>\n",
       "      <th>author_id</th>\n",
       "      <th>state</th>\n",
       "      <th>year</th>\n",
       "      <th>dateFiled</th>\n",
       "      <th>citeCount</th>\n",
       "      <th>snippet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fla</td>\n",
       "      <td>4019.0</td>\n",
       "      <td>FL</td>\n",
       "      <td>1973</td>\n",
       "      <td>1973-07-26T00:00:00Z</td>\n",
       "      <td>552</td>\n",
       "      <td>whether the death penalty per unconstitutional...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>texcrimapp</td>\n",
       "      <td>5765.0</td>\n",
       "      <td>TX</td>\n",
       "      <td>1975</td>\n",
       "      <td>1975-04-16T00:00:00Z</td>\n",
       "      <td>143</td>\n",
       "      <td>contention that the assessment the death penal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>texcrimapp</td>\n",
       "      <td>5758.0</td>\n",
       "      <td>TX</td>\n",
       "      <td>1944</td>\n",
       "      <td>1944-12-20T00:00:00Z</td>\n",
       "      <td>56</td>\n",
       "      <td>assume the district attorney orally waived the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>azd</td>\n",
       "      <td>550.0</td>\n",
       "      <td>AZ</td>\n",
       "      <td>2003</td>\n",
       "      <td>2003-05-19T00:00:00Z</td>\n",
       "      <td>0</td>\n",
       "      <td>against death penalty stop prisoner rape citiz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>texcrimapp</td>\n",
       "      <td>5765.0</td>\n",
       "      <td>TX</td>\n",
       "      <td>1964</td>\n",
       "      <td>1964-10-14T00:00:00Z</td>\n",
       "      <td>80</td>\n",
       "      <td>this court received the record death penalty c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     court_id  author_id state  year             dateFiled  citeCount  \\\n",
       "1         fla     4019.0    FL  1973  1973-07-26T00:00:00Z        552   \n",
       "2  texcrimapp     5765.0    TX  1975  1975-04-16T00:00:00Z        143   \n",
       "4  texcrimapp     5758.0    TX  1944  1944-12-20T00:00:00Z         56   \n",
       "5         azd      550.0    AZ  2003  2003-05-19T00:00:00Z          0   \n",
       "9  texcrimapp     5765.0    TX  1964  1964-10-14T00:00:00Z         80   \n",
       "\n",
       "                                             snippet  \n",
       "1  whether the death penalty per unconstitutional...  \n",
       "2  contention that the assessment the death penal...  \n",
       "4  assume the district attorney orally waived the...  \n",
       "5  against death penalty stop prisoner rape citiz...  \n",
       "9  this court received the record death penalty c...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check the data\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 18215 entries, 1 to 32566\n",
      "Data columns (total 7 columns):\n",
      "court_id     18215 non-null object\n",
      "author_id    18215 non-null float64\n",
      "state        18215 non-null object\n",
      "year         18215 non-null int64\n",
      "dateFiled    18215 non-null object\n",
      "citeCount    18215 non-null int64\n",
      "snippet      18215 non-null object\n",
      "dtypes: float64(1), int64(2), object(4)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    "#check the data types etc\n",
    "df1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the .pkl files\n",
    "vocab = pd.read_pickle('vocab.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information on .pkl files in Python, please check this link below https://pythonnumericalmethods.berkeley.edu/notebooks/chapter11.03-Pickle-Files.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean the text data\n",
    "translator = str.maketrans('','',punctuation) \n",
    "def fix_snippet(txt):\n",
    "    a = txt.encode(\"ascii\", errors=\"ignore\").decode()\n",
    "    a = re.sub('\\W\\w\\W', ' ', a).lower()\n",
    "    a = re.sub('\\W\\w\\w\\W', ' ', a)\n",
    "    a = a.replace('&quot;', ' ').replace ('\\n', ' ')\n",
    "    a = a.translate(translator)\n",
    "    a = a.replace('deathpenalty',' ')\n",
    "    a = ' '.join(a.split())\n",
    "    return a\n",
    "df1['snippet'] = df1['snippet'].apply(fix_snippet)\n",
    "df1['snippet'] = df1['snippet'].apply(fix_snippet)\n",
    "df1.to_csv('cases-processed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entity Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make judge dummy variables\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "J = encoder.fit_transform(df1['author_id'].astype(str))\n",
    "num_judges = max(J)+1\n",
    "Y = df1['citeCount'] > 0\n",
    "Y2 = np.log(1+df1['citeCount'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Failed to import `pydot`. Please install `pydot`. For example with `pip install pydot`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-fe717d0178db>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m dot = model_to_dot(model,\n\u001b[0;32m     17\u001b[0m                    \u001b[0mshow_shapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m                    show_layer_names=False)\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[0mSVG\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprog\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'dot'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'svg'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\utils\\vis_utils.py\u001b[0m in \u001b[0;36mmodel_to_dot\u001b[1;34m(model, show_shapes, show_layer_names, rankdir)\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m     \u001b[0m_check_pydot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m     \u001b[0mdot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpydot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[0mdot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'rankdir'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrankdir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\utils\\vis_utils.py\u001b[0m in \u001b[0;36m_check_pydot\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mpydot\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         raise ImportError(\n\u001b[1;32m---> 20\u001b[1;33m             \u001b[1;34m'Failed to import `pydot`. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m             \u001b[1;34m'Please install `pydot`. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m             'For example with `pip install pydot`.')\n",
      "\u001b[1;31mImportError\u001b[0m: Failed to import `pydot`. Please install `pydot`. For example with `pip install pydot`."
     ]
    }
   ],
   "source": [
    "# Set up Deep Neural Networks (DNN)\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Flatten\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(num_judges, # number of categories\n",
    "                    2, # dimensions of embedding\n",
    "                    input_length=1)) \n",
    "model.add(Flatten()) # needed after Embedding\n",
    "model.add(Dense(2))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "dot = model_to_dot(model,\n",
    "                   show_shapes=True,\n",
    "                   show_layer_names=False)\n",
    "SVG(dot.create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 1, 2)              5964      \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 6         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 3         \n",
      "=================================================================\n",
      "Total params: 5,973\n",
      "Trainable params: 5,973\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install ggplot to visualize vectors (using Prompt/Terminal) https://pypi.org/project/ggplot/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ggplot'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-0d9d770754f2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Visualize the Judge Vectors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mggplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mgg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mjudge_cites\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mJ\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdf2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mJ\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'judge'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop_duplicates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'judge'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdf2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'cites'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'judge'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mjudge_cites\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'ggplot'"
     ]
    }
   ],
   "source": [
    "# Visualize the Judge Vectors\n",
    "import ggplot as gg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "judge_cites = dict(Y.groupby(J).mean())\n",
    "df2 = pd.DataFrame(J,columns=['judge']).drop_duplicates().sort_values('judge')\n",
    "df2['cites'] = df2['judge'].apply(lambda x: judge_cites[x])\n",
    "\n",
    "for i in range(5):\n",
    "    if i > 0:\n",
    "        model.fit(J,Y,epochs=1, validation_split=.2)\n",
    "    \n",
    "    judge_vectors = model.layers[0].get_weights()[0]\n",
    "    df2['x'] = judge_vectors[:,0]\n",
    "    df2['y'] = judge_vectors[:,1]    \n",
    "    chart = gg.ggplot( df2, gg.aes(x='x', y='y', color='cites') ) \\\n",
    "                      + gg.geom_point(size=10, alpha=.8) \n",
    "    chart.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert documents to sequences of word indexes\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "num_words = 200\n",
    "tokenizer = Tokenizer(num_words=num_words)\n",
    "tokenizer.fit_on_texts(df1['snippet'])\n",
    "sequences = tokenizer.texts_to_sequences(df1['snippet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# represent data as numrows x maxlen matrix\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "maxlen = max([len(sent) for sent in sequences]) \n",
    "maxlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pad_sequences(sequences, maxlen=maxlen)\n",
    "\n",
    "# Model setup\n",
    "model = Sequential()\n",
    "model.add(Embedding(num_words,\n",
    "                    2,\n",
    "                    input_length=maxlen)) # sequence length\n",
    "model.add(Flatten()) # 86*2 = 172 dims\n",
    "model.add(Dense(2))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy')\n",
    "dot = model_to_dot(model, show_shapes=True, show_layer_names=False)\n",
    "SVG(dot.create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the vectors\n",
    "df3 = pd.DataFrame(list(tokenizer.word_index.items()),\n",
    "                  columns=['word', 'word_index']).sort_values('word_index')[:num_words]\n",
    "\n",
    "for i in range(5):\n",
    "    if i > 0:\n",
    "        model.fit(X,Y,epochs=1, validation_split=.2)\n",
    "\n",
    "    word_vectors = model.layers[0].get_weights()[0]\n",
    "    df3['x'] = word_vectors[:,0]\n",
    "    df3['y'] = word_vectors[:,1]\n",
    "    chart = gg.ggplot( df3, gg.aes(x='x', y='y', label='word') ) \\\n",
    "                      + gg.geom_text(size=10, alpha=.8, label='word') \n",
    "    chart.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Similarity\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "vec_defendants = word_vectors[tokenizer.word_index['defendants']-1]\n",
    "vec_convicted = word_vectors[tokenizer.word_index['convicted']-1]\n",
    "vec_against = word_vectors[tokenizer.word_index['against']-1]\n",
    "\n",
    "print(1-cosine(vec_defendants, vec_convicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(1-cosine(vec_defendants, vec_against))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2vec in Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2vec requires sentences as input\n",
    "from utils import get_sentences\n",
    "sentences = []\n",
    "for doc in df1['snippet']:\n",
    "    sentences += get_sentences(doc)\n",
    "from random import shuffle\n",
    "shuffle(sentences) # stream in sentences in random order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "from gensim.models import Word2Vec\n",
    "w2v = Word2Vec(sentences,  # list of tokenized sentences\n",
    "               workers = 8, # Number of threads to run in parallel\n",
    "               size=300,  # Word vector dimensionality     \n",
    "               min_count =  25, # Minimum word count  \n",
    "               window = 5, # Context window size      \n",
    "               sample = 1e-3, # Downsample setting for frequent words\n",
    "               )\n",
    "\n",
    "# done training, so delete context vectors\n",
    "w2v.init_sims(replace=True)\n",
    "\n",
    "w2v.save('w2v-vectors.pkl')\n",
    "\n",
    "w2v.wv['judg'] # vector for \"judge\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check similarity between these two words\n",
    "w2v.wv.similarity('judg','juri') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find most similar words to 'judg'\n",
    "w2v.wv.most_similar('judg') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analogies: judge is to man as __ is to woman\n",
    "w2v.wv.most_similar(positive=['judg','man'],\n",
    "                 negative=['woman'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec: K-Means Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmw = KMeans(n_clusters=50)\n",
    "kmw.fit(w2v.wv.vectors)\n",
    "judge_clust = kmw.labels_[w2v.wv.vocab['judg'].index]\n",
    "for i, cluster in enumerate(kmw.labels_):\n",
    "    if cluster == judge_clust:\n",
    "        print(w2v.wv.index2word[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# Pre-trained vectors\n",
    "###\n",
    "\n",
    "import spacy\n",
    "en = spacy.load('en_core_web_lg')\n",
    "apple = en('apple') \n",
    "apple.vector # vector for 'apple'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check how similar the word 'apple' is to 'apple'\n",
    "apple.similarity(apple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orange = en('orange')\n",
    "apple.similarity(orange)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "it = spacy.load('it')\n",
    "mela = it('mela')\n",
    "arancia = it('arancia')\n",
    "mela.similarity(arancia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing an embedding layer with pre-trained vectors\n",
    "embed_dims = len(apple.vector)\n",
    "embedding_matrix = np.zeros([num_words, embed_dims])\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i > num_words:\n",
    "        break\n",
    "    embedding_vector = en(word).vector\n",
    "    embedding_matrix[i-1] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(num_words,\n",
    "                    embed_dims,\n",
    "                    weights=[embedding_matrix],\n",
    "                    input_length=maxlen,\n",
    "                    trainable=False)) # frozen layer\n",
    "model.add(Flatten()) # 86*300 = 25800 dims\n",
    "model.add(Dense(64,activation='relu'))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the vectors\n",
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(n_components=2, verbose=1, perplexity=50, n_iter=300)\n",
    "\n",
    "df3 = pd.DataFrame(list(tokenizer.word_index.items()),\n",
    "                  columns=['word', 'word_index']).sort_values('word_index')[:num_words]\n",
    "\n",
    "for i in range(3):\n",
    "    if i > 0:\n",
    "        model.fit(X,Y,epochs=1, validation_split=.2)\n",
    "    \n",
    "    word_vectors = model.layers[0].get_weights()[0]\n",
    "    wv_tsne = tsne.fit_transform(word_vectors)\n",
    "\n",
    "    df3['x'] = wv_tsne[:,0]\n",
    "    df3['y'] = wv_tsne[:,1]\n",
    "    chart = gg.ggplot( df3, gg.aes(x='x', y='y', label='word') ) \\\n",
    "                      + gg.geom_text(size=10, alpha=.8, label='word') \n",
    "    chart.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Mover Distance\n",
    "###\n",
    "\n",
    "import spacy\n",
    "import wmd\n",
    "nlp = spacy.load('en', \n",
    "                 create_pipeline=wmd.WMD.create_spacy_pipeline)\n",
    "doc1 = nlp(\"Politician speaks to the media in Illinois.\")\n",
    "doc2 = nlp(\"The president greets the press in Chicago.\")\n",
    "print(doc1.similarity(doc2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
