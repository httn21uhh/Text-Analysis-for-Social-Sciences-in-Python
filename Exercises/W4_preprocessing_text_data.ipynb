{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "W4_preprocessing_text_data.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOZxRRheoiZs"
      },
      "source": [
        "#Important packages to clean and preprocess texts\n",
        "import re \n",
        "import string\n",
        "from nltk.tokenize import word_tokenize \n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer \n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_IKSTATocMi"
      },
      "source": [
        "#Removing numbers\n",
        "#Using regex (i.e. regular expressions)\n",
        "text_1 = \"The weather is exceptionally nice today. Except for the fact that on Thursday, we will have 5 thunderstorms. Oh well, but who cares?\"\n",
        "text_clean = re.sub(r\"\\d+\", \"\",text_1)\n",
        "print(text_clean)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWmYiG_yo94R"
      },
      "source": [
        "#Remove white space\n",
        "text_1 = \" \\t I love Corgis. They are the     cutest \"\n",
        "text_clean = text_1.strip()\n",
        "print(text_clean)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-FR5N9QpOjG"
      },
      "source": [
        "#Separating Sentences with Split () Method\n",
        "sent = \"The weather is exceptionally nice today. Except for the fact that on Thursday, we will have 5 thunderstorms. Oh well, but who cares?\"\n",
        "sent_split = sent.split()\n",
        "print(sent_split)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oz4SVeB9piK8",
        "outputId": "03d4c542-e919-4086-aac2-99dad25e1654"
      },
      "source": [
        "#Tokenizing words in sentences with NLTK\n",
        "sent = \"The weather is exceptionally nice today. Except for the fact that on Thursday, we will have 5 thunderstorms. Oh well, but who cares?\"\n",
        "\n",
        "tokens = nltk.tokenize.word_tokenize(sent)\n",
        "print(tokens)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'weather', 'is', 'exceptionally', 'nice', 'today', '.', 'Except', 'for', 'the', 'fact', 'that', 'on', 'Thursday', ',', 'we', 'will', 'have', '5', 'thunderstorms', '.', 'Oh', 'well', ',', 'but', 'who', 'cares', '?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFxWzqkvp5xU"
      },
      "source": [
        "# Removing stopwords\n",
        "sent = \"The weather is exceptionally nice today. Except for the fact that on Thursday, we will have 5 thunderstorms. Oh well, but who cares? I think the best kind of weather is an unpredictable one like this one we have in Hamburg.\"\n",
        "sent = sent.translate(str.maketrans('','',string.punctuation)).lower()\n",
        " \n",
        "tokens = word_tokenize(sent)\n",
        "listStopword =  set(stopwords.words('english'))\n",
        " \n",
        "removed = []\n",
        "for t in tokens:\n",
        "    if t not in listStopword:\n",
        "        removed.append(t)\n",
        " \n",
        "print(removed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPBgswygqlUm"
      },
      "source": [
        "#Porter stemmer can easily be used as follows\n",
        "ps = PorterStemmer() \n",
        "\n",
        "word_program = [\"program\", \"programs\", \"programer\", \"programing\", \"programers\"] \n",
        "  \n",
        "for k in word_program: \n",
        "    print(k, \" : \", ps.stem(k))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3rU7jywq0WR"
      },
      "source": [
        "You can find more information on the official website for Porter stemmer algorithm here:\n",
        "https://tartarus.org/martin/PorterStemmer/index-old.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMVUcWBPq9l-"
      },
      "source": [
        "# **PRE-PROCESSING ON REAL DATA SET**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7_E0ftag0zz"
      },
      "source": [
        "We will model the approach on the **Covid-19 Twitter dataset**. Our main task is to clean and filer out English tweets only.\n",
        "\n",
        "The original dataset is by Preda, G. (2020, August 30). COVID19 Tweets. Retrieved from https://www.kaggle.com/gpreda/covid19-tweets\n",
        "\n",
        "There are 3 major components to this approach:\n",
        "\n",
        "1.  Clean and filter all non-English tweets/texts as we want consistency in the data.\n",
        " \n",
        "2.   Create a simplified version for our complex text data.\n",
        "\n",
        "3.   Vectorize the text and save their embedding for future analysis.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnaFiYHch_p8"
      },
      "source": [
        "### **PART 0: Loading packages and the dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k85dCqifhT0T",
        "outputId": "157de46b-83e2-474d-9e69-3cd1cf0203a6"
      },
      "source": [
        "#Import necessery libraries for future analysis of the dataset\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "np.random.seed(2020)\n",
        "import nltk\n",
        "nltk.download('punkt') # one time execution\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWWbPB-sh1yk"
      },
      "source": [
        "data = pd.read_csv(\"covid19_tweets.csv\")\n",
        "data.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9nBCg4uh4wc"
      },
      "source": [
        "\n",
        "data.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3AkOouUh5Ti"
      },
      "source": [
        "\n",
        "data.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHbBIVTLh7Me"
      },
      "source": [
        "data.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YwmZIkNmh8wY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K36A6MpgiLLY"
      },
      "source": [
        "### **PART 1:Data cleaning**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TW80PbeSi8Ri"
      },
      "source": [
        "#Clean and standardize text to English-only tweets\n",
        "#This function will remove all non-English characters\n",
        "# no non-english\n",
        "def clean_non_english(txt): \n",
        "    try: \n",
        "        txt = re.sub(r'\\W+', ' ', txt)\n",
        "        txt = txt.lower()\n",
        "        txt = txt.replace(\"[^a-zA-Z]\", \" \")\n",
        "        word_tokens = word_tokenize(txt) \n",
        "        filtered_word = [w for w in word_tokens if all(ord(c) < 128 for c in w)]\n",
        "        filtered_word = [w + \" \" for w in filtered_word]\n",
        "        return \"\".join(filtered_word)\n",
        "    except:\n",
        "        return np.nan\n",
        "\n",
        "data[\"english_text\"] = data.text.apply(clean_non_english)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k38jiF-sjJvA"
      },
      "source": [
        "# all clean\n",
        "def clean_text(english_txt): \n",
        "    try: \n",
        "        word_tokens = word_tokenize(english_txt)\n",
        "        filtered_word = [w for w in word_tokens if not w in stop_words] \n",
        "        filtered_word = [w + \" \" for w in filtered_word]\n",
        "        return \"\".join(filtered_word)\n",
        "    except:\n",
        "        return np.nan\n",
        "\n",
        "data[\"cleaned_text\"] = data.english_text.apply(clean_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tvcXO7TpjNSJ"
      },
      "source": [
        "data.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnnvI8aXjdh7"
      },
      "source": [
        "We can even do better by removing the stopwords. \n",
        "\n",
        "Stopwords are common words that appear in English sentences without contributing much to the meaning. We will use the nltk package to filter the stopwords. \n",
        "\n",
        "As our main task is visualizing the common theme of tweets using word cloud, this step is necessary to avoid common words like “*the*,” “*a*,”,“*is*”, etc.\n",
        "\n",
        "However, if your tasks require full sentence structure, like next word prediction or grammar check, you can skip this step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0AAqNbXWkAPp"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt') # one time execution\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "def clean_text(english_txt):\n",
        "    try:\n",
        "       word_tokens = word_tokenize(english_txt)\n",
        "       filtered_word = [w for w in word_tokens if not w in stop_words]\n",
        "       filtered_word = [w + \" \" for w in filtered_word]\n",
        "       return \"\".join(filtered_word)\n",
        "    except:\n",
        "       return np.nan"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTs4rdtwit6m"
      },
      "source": [
        "For tweets, there is a special feature we need to consider before cleaning: mentions. Your data might have special features like this (or not). Therefore, this is case-by-case and NOT a universal requirement.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "utjjAWePiTVX"
      },
      "source": [
        "def get_mention(txt):\n",
        "    mention = []\n",
        "    for i in txt.split(\" \"):\n",
        "        if len(i) > 0 and i[0] == \"@\":\n",
        "            mention.append(i)\n",
        "    return \"\".join([mention[i] + \", \" if i != len(mention) - 1 else mention[i] for i in range(len(mention))])\n",
        "\n",
        "data[\"mention\"] = data.text.apply(get_mention)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CufK3M5Ti2gq"
      },
      "source": [
        "def remove_link_email(txt):\n",
        "    txt = txt.replace(\"...\", \"\")\n",
        "    txt = re.sub(r\"http\\S+\", \"\", txt)\n",
        "    txt = txt.replace('\\S*@\\S*\\s?', \"\")\n",
        "    txt = re.sub(r'[^\\w\\s]', '', txt)\n",
        "    return txt\n",
        "\n",
        "data.text = data.text.apply(remove_link_email)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hG0DCkk-kXFL"
      },
      "source": [
        "We then filter out all columns then is not *“en”* language.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "biwrIK40kZQD"
      },
      "source": [
        "def clean_tag(txt):\n",
        "    try:\n",
        "        chars = \"'[]\"\n",
        "        for char in chars:\n",
        "            txt = txt.replace(char, \"\")\n",
        "        txt = txt.lower()\n",
        "        return txt\n",
        "    except: \n",
        "        return np.nan\n",
        "\n",
        "data[\"cleaned_tags\"] = data.hashtags.apply(clean_tag)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2o6SxnFmh5-"
      },
      "source": [
        "\n",
        "covid_list = []\n",
        "for item in data.cleaned_tags:\n",
        "    try:\n",
        "        if item != np.nan:\n",
        "            covid_list_word = item.split(\", \")\n",
        "        covid_list += covid_list_word\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "from collections import Counter\n",
        "x = Counter(covid_list)\n",
        "x.most_common(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gRWKkbPmvCy"
      },
      "source": [
        "def get_len_hashtag(txt):\n",
        "    try: \n",
        "        return len(txt.split(\",\"))\n",
        "    except:\n",
        "        return np.nan\n",
        "\n",
        "data[\"len_hashtag\"] = data.hashtags.apply(get_len_hashtag)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUNC1f4qmw5a"
      },
      "source": [
        "\n",
        "data.dropna(subset=[\"user_description\", \"user_location\", \"hashtags\", \"cleaned_text\", \"text\", \"english_text\", 'cleaned_tags'], inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pg5RlWLWmyiH"
      },
      "source": [
        "data.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNWZHzHHm2cJ"
      },
      "source": [
        "\n",
        "data = data.sample(n = 10000)\n",
        "data.reset_index(inplace=True)\n",
        "data.drop(['index', 'source'], axis = 1, inplace=True)\n",
        "print(data.shape)\n",
        "data.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94Elk-2Bm4ub"
      },
      "source": [
        "data.isnull().sum()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TaRy05ICkOF0"
      },
      "source": [
        "Before, we clean the non-English characters. Now, we remove the non-English texts (semantically). Langdetect is a python package that allows for checking the language of the text. It is a direct port of Google’s language detection library from Java to Python."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdgWTkOikPt6"
      },
      "source": [
        "from langdetect import detect\n",
        "def detect_lang(txt):\n",
        "    try:\n",
        "        return detect(txt)\n",
        "    except:\n",
        "        return np.nan"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5uVgK5HnHxQ"
      },
      "source": [
        "new_data = data[data.language == \"en\"]\n",
        "new_data.reset_index(inplace = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ViTuyqJInJvW"
      },
      "source": [
        "\n",
        "new_data.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZR-1oViWnMF4"
      },
      "source": [
        "new_data.drop(['index'], axis=1, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWNwxPoanN2A"
      },
      "source": [
        "new_data.to_csv('english_tweets.csv')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}